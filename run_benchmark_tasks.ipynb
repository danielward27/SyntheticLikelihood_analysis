{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note this script needs a conda environment with sbibm installed, e.g.:\n",
    "\n",
    "# using Conda\n",
    "# using Pkg\n",
    "\n",
    "# ENV[\"PYTHON\"] = \"\"\n",
    "# Pkg.build(\"PyCall\")\n",
    "\n",
    "# Conda.pip_interop(true)\n",
    "# Conda.pip(\"install\", \"sbibm\")\n",
    "\n",
    "# Make sure up to date\n",
    "# Pkg.rm(\"SyntheticLikelihood\")\n",
    "# Pkg.add(url=\"https://github.com/danielward27/SyntheticLikelihood.jl\")\n",
    "using SyntheticLikelihood\n",
    "using PyCall\n",
    "using Distributions\n",
    "using DelimitedFiles\n",
    "using Random\n",
    "using Parameters\n",
    "using LinearAlgebra\n",
    "\n",
    "sbibm = pyimport(\"sbibm\")\n",
    "torch = pyimport(\"torch\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert stuff from python to julia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"gaussian_linear\", \"gaussian_linear_uniform\", \"gaussian_mixture\", \"sir\", \"bernoulli_glm\")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_priors = include(\"task_priors.jl\")\n",
    "String.(keys(task_priors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rough test that prior conversion looks right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gaussian_linear\n",
      "Julia means = [0.0, 0.0, 0.01, 0.01, 0.01, 0.0, -0.01, -0.0, 0.0, -0.01]\n",
      "Python means = Float32[-0.01, -0.0, 0.02, 0.0, -0.0, -0.0, 0.0, 0.0, 0.01, -0.01] \n",
      "\n",
      "gaussian_linear_uniform\n",
      "Julia means = [0.01, -0.01, 0.0, -0.0, 0.01, 0.01, -0.0, 0.0, 0.0, 0.02]\n",
      "Python means = Float32[-0.01, -0.01, -0.0, 0.01, 0.01, 0.0, 0.01, 0.01, -0.02, -0.01] \n",
      "\n",
      "gaussian_mixture\n",
      "Julia means = [-0.04, -0.04]\n",
      "Python means = Float32[-0.03, -0.18] \n",
      "\n",
      "sir\n",
      "Julia means = [0.45, 0.13]\n",
      "Python means = Float32[0.45, 0.13] \n",
      "\n",
      "bernoulli_glm\n",
      "Julia means = [-0.04, 0.01, -0.0, 0.0, 0.02, 0.03, 0.01, -0.0, -0.03, -0.02]\n",
      "Python means = Float32[0.0, 0.03, 0.07, 0.06, 0.02, -0.01, -0.02, -0.03, -0.02, 0.0] \n",
      "\n",
      "bernoulli_glm_raw\n",
      "Julia means = [0.03, 0.02, 0.04, 0.04, 0.05, 0.05, 0.02, -0.0, -0.02, -0.01]\n",
      "Python means = Float32[-0.01, -0.02, 0.01, -0.0, -0.02, -0.02, -0.02, -0.01, -0.02, -0.02] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for task_name in String.(keys(task_priors))\n",
    "    n = 2000\n",
    "    jl_prior = task_priors[Symbol(task_name)]\n",
    "    jl_samples = sample_θ(jl_prior, n)\n",
    "    jl_mean = mean.(eachcol(jl_samples))\n",
    "    jl_cov = cov(jl_samples)\n",
    "\n",
    "    py_prior = sbibm.get_task(task_name).get_prior()\n",
    "    py_samples = py_prior(n).numpy();\n",
    "    py_mean = mean.(eachcol(py_samples))\n",
    "    py_cov = cov(py_samples)\n",
    "    \n",
    "    println(task_name)\n",
    "    println(\"Julia means = $(round.(jl_mean; digits = 2))\")\n",
    "    println(\"Python means = $(round.(py_mean; digits = 2)) \\n\")\n",
    "    \n",
    "    @assert size(py_mean) == size(jl_mean)\n",
    "    @assert isapprox(py_mean, jl_mean; rtol = 2)\n",
    "    @assert isapprox(py_cov, jl_cov; rtol = 0.7)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_jl_simulator(task)   \n",
    "    py_simulator = task.get_simulator()\n",
    "    simulator(θ::Vector{Float64}) = begin\n",
    "        θ = torch.tensor(θ, dtype = torch.float32)\n",
    "        x = py_simulator(θ)\n",
    "        convert(Vector{Float64}, vec(x.numpy()))\n",
    "    end\n",
    "    simulator\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct JuliaTask\n",
    "    name\n",
    "    simulator\n",
    "    prior\n",
    "    s_true\n",
    "    obs_seed\n",
    "end\n",
    "\n",
    "function JuliaTask(python_task, obs_seed::Integer)\n",
    "    name = python_task.name\n",
    "    simulator = get_jl_simulator(python_task)\n",
    "    prior = task_priors[Symbol(name)]\n",
    "    s_true = vec(python_task.get_observation(obs_seed).numpy())\n",
    "    s_true = convert(Vector{Float64}, s_true)\n",
    "    JuliaTask(name, simulator, prior, s_true, obs_seed)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop through tasks and run the Riemannian ULA algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADD IN RUN TIMES BELOW!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Task = gaussian_linear\n",
      "└ @ Main In[9]:2\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:24:19\u001b[39m\n",
      "┌ Info: Task = gaussian_linear_uniform\n",
      "└ @ Main In[9]:2\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:25:39\u001b[39m\n",
      "┌ Info: Task = gaussian_mixture\n",
      "└ @ Main In[9]:2\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:12:03\u001b[39m\n",
      "┌ Info: Task = sir\n",
      "└ @ Main In[9]:2\n",
      "\u001b[32mProgress:   2%|█                                        |  ETA: 2:23:19\u001b[39m┌ Warning: GLM did not converge. Corresponding variance set to sample\n",
      "│ covariance.\n",
      "└ @ SyntheticLikelihood /home/dw16200/.julia/packages/SyntheticLikelihood/NjVDE/src/glm_local_regression.jl:69\n",
      "\u001b[32mProgress:   5%|██                                       |  ETA: 2:18:34\u001b[39m┌ Warning: GLM did not converge. Corresponding variance set to sample\n",
      "│ covariance.\n",
      "└ @ SyntheticLikelihood /home/dw16200/.julia/packages/SyntheticLikelihood/NjVDE/src/glm_local_regression.jl:69\n",
      "\u001b[32mProgress:  15%|██████▎                                  |  ETA: 2:02:32\u001b[39m┌ Warning: GLM did not converge. Corresponding variance set to sample\n",
      "│ covariance.\n",
      "└ @ SyntheticLikelihood /home/dw16200/.julia/packages/SyntheticLikelihood/NjVDE/src/glm_local_regression.jl:69\n",
      "\u001b[32mProgress:  17%|██████▊                                  |  ETA: 2:00:42\u001b[39m┌ Warning: GLM did not converge. Corresponding variance set to sample\n",
      "│ covariance.\n",
      "└ @ SyntheticLikelihood /home/dw16200/.julia/packages/SyntheticLikelihood/NjVDE/src/glm_local_regression.jl:69\n",
      "\u001b[32mProgress:  23%|█████████▋                               |  ETA: 1:50:41\u001b[39m┌ Warning: GLM did not converge. Corresponding variance set to sample\n",
      "│ covariance.\n",
      "└ @ SyntheticLikelihood /home/dw16200/.julia/packages/SyntheticLikelihood/NjVDE/src/glm_local_regression.jl:69\n",
      "\u001b[32mProgress:  26%|██████████▋                              |  ETA: 1:46:49\u001b[39m┌ Warning: GLM did not converge. Corresponding variance set to sample\n",
      "│ covariance.\n",
      "└ @ SyntheticLikelihood /home/dw16200/.julia/packages/SyntheticLikelihood/NjVDE/src/glm_local_regression.jl:69\n",
      "\u001b[32mProgress:  28%|███████████▎                             |  ETA: 1:44:27\u001b[39m┌ Warning: GLM did not converge. Corresponding variance set to sample\n",
      "│ covariance.\n",
      "└ @ SyntheticLikelihood /home/dw16200/.julia/packages/SyntheticLikelihood/NjVDE/src/glm_local_regression.jl:69\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 2:22:09\u001b[39m\n",
      "┌ Info: Task = bernoulli_glm\n",
      "└ @ Main In[9]:2\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:25:28\u001b[39m\n",
      "┌ Info: Task = bernoulli_glm_raw\n",
      "└ @ Main In[9]:2\n",
      "┌ Warning: GLM did not converge. Corresponding variance set to sample\n",
      "│ covariance.\n",
      "└ @ SyntheticLikelihood /home/dw16200/.julia/packages/SyntheticLikelihood/NjVDE/src/glm_local_regression.jl:69\n",
      "┌ Warning: GLM did not converge. Corresponding variance set to sample\n",
      "│ covariance.\n",
      "└ @ SyntheticLikelihood /home/dw16200/.julia/packages/SyntheticLikelihood/NjVDE/src/glm_local_regression.jl:69\n",
      "┌ Warning: GLM did not converge. Corresponding variance set to sample\n",
      "│ covariance.\n",
      "└ @ SyntheticLikelihood /home/dw16200/.julia/packages/SyntheticLikelihood/NjVDE/src/glm_local_regression.jl:69\n",
      "┌ Warning: GLM did not converge. Corresponding variance set to sample\n",
      "│ covariance.\n",
      "└ @ SyntheticLikelihood /home/dw16200/.julia/packages/SyntheticLikelihood/NjVDE/src/glm_local_regression.jl:69\n",
      "┌ Warning: GLM did not converge. Corresponding variance set to sample\n",
      "│ covariance.\n",
      "└ @ SyntheticLikelihood /home/dw16200/.julia/packages/SyntheticLikelihood/NjVDE/src/glm_local_regression.jl:69\n",
      "┌ Warning: GLM did not converge. Corresponding variance set to sample\n",
      "│ covariance.\n",
      "└ @ SyntheticLikelihood /home/dw16200/.julia/packages/SyntheticLikelihood/NjVDE/src/glm_local_regression.jl:69\n",
      "┌ Warning: GLM did not converge. Corresponding variance set to sample\n",
      "│ covariance.\n",
      "└ @ SyntheticLikelihood /home/dw16200/.julia/packages/SyntheticLikelihood/NjVDE/src/glm_local_regression.jl:69\n",
      "┌ Warning: GLM did not converge. Corresponding variance set to sample\n",
      "│ covariance.\n",
      "└ @ SyntheticLikelihood /home/dw16200/.julia/packages/SyntheticLikelihood/NjVDE/src/glm_local_regression.jl:69\n",
      "┌ Warning: bernoulli_glm_raw failed!\n",
      "└ @ Main In[9]:28\n"
     ]
    }
   ],
   "source": [
    "for (i, task_name) in enumerate(String.(keys(task_priors)))\n",
    "    @info \"Task = $(task_name)\"\n",
    "\n",
    "    Random.seed!(i)\n",
    "    pytask = sbibm.get_task(task_name)\n",
    "    jltask = JuliaTask(pytask, 1)\n",
    "    \n",
    "    @unpack simulator, prior, s_true, obs_seed = jltask\n",
    "    \n",
    "    n_steps = 4000\n",
    "    init_θ = sample_θ(prior)\n",
    "\n",
    "    local_posterior = LocalPosterior(;\n",
    "      simulator,\n",
    "      s_true,\n",
    "      n_sim = 1000,\n",
    "      prior,\n",
    "    )\n",
    "    \n",
    "    rula = RiemannianULA(0.2)\n",
    "    \n",
    "    try\n",
    "        data = run_sampler!(rula, local_posterior; init_θ, n_steps)\n",
    "        open(\"./samples/$(task_name)_rula.txt\", \"w\") do io\n",
    "            writedlm(io, data.θ)\n",
    "        end\n",
    "    catch e\n",
    "        @warn \"$(task_name) failed!\"\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop through tasks and run basic Bayesian Synthetic Likelihood\n",
    "Below we use standard synthetic likelihood. We use a burn in of 1000 samples, and then use the empirical covariance matrix of the last 500 samples of the burn in to form the proposal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_posterior = BasicPosterior(; simulator, s_true, prior)\n",
    "rwm = RWMetropolis(MvNormal(cov(expected)))\n",
    "\n",
    "data = run_sampler!(rwm, basic_posterior; init_θ, n_steps = 4000,\n",
    "  collect_data = [:θ, :accepted])\n",
    "\n",
    "θ = data.θ\n",
    "@test isapprox(mean(expected), mean.(eachcol(θ)); atol = 2)\n",
    "acceptance_rate = data.accepted[end]/1000\n",
    "\n",
    "# hcat(unique.(eachcol(θ))...)\n",
    "110/1000\n",
    "\n",
    "using Plots\n",
    "plot(θ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using DelimitedFiles\n",
    "#using GLM\n",
    "#X = readdlm(\"X.txt\")\n",
    "#y = readdlm(\"y.txt\")\n",
    "#y = reshape(y, length(y))\n",
    "#glm(X, y, Gamma(), LogLink(), maxiter=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.0",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
