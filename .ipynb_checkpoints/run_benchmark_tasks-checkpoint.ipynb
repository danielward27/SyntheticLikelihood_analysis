{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note this script needs a conda environment with sbibm installed, e.g.:\n",
    "\n",
    "# using Conda\n",
    "# using Pkg\n",
    "\n",
    "# ENV[\"PYTHON\"] = \"\"\n",
    "# Pkg.build(\"PyCall\")\n",
    "\n",
    "# Conda.pip_interop(true)\n",
    "# Conda.pip(\"install\", \"sbibm\")\n",
    "\n",
    "# Make sure up to date\n",
    "# Pkg.rm(\"SyntheticLikelihood\")\n",
    "# Pkg.add(url=\"https://github.com/danielward27/SyntheticLikelihood.jl\")\n",
    "using SyntheticLikelihood\n",
    "using PyCall\n",
    "using Distributions\n",
    "using DelimitedFiles\n",
    "using Random\n",
    "using Parameters\n",
    "using LinearAlgebra\n",
    "using DataFrames\n",
    "using CSV\n",
    "\n",
    "sbibm = pyimport(\"sbibm\")\n",
    "torch = pyimport(\"torch\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert stuff from python to julia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"gaussian_linear\", \"gaussian_linear_uniform\", \"gaussian_mixture\", \"sir\", \"bernoulli_glm\")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_priors = include(\"task_priors.jl\")\n",
    "String.(keys(task_priors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rough test that prior conversion looks right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gaussian_linear\n",
      "Julia means = [0.0, -0.0, 0.0, 0.01, -0.0, 0.0, 0.0, -0.0, -0.01, -0.0]\n",
      "Python means = Float32[0.0, 0.01, -0.01, 0.01, -0.0, 0.0, -0.02, 0.01, 0.01, 0.01] \n",
      "\n",
      "gaussian_linear_uniform\n",
      "Julia means = [0.01, -0.0, -0.0, -0.01, -0.01, -0.01, -0.0, 0.0, -0.0, 0.02]\n",
      "Python means = Float32[-0.01, -0.0, 0.01, -0.01, -0.0, -0.0, 0.01, -0.01, -0.0, 0.0] \n",
      "\n",
      "gaussian_mixture\n",
      "Julia means = [0.27, 0.07]\n",
      "Python means = Float32[-0.0, 0.02] \n",
      "\n",
      "sir\n",
      "Julia means = [0.46, 0.13]\n",
      "Python means = Float32[0.45, 0.13] \n",
      "\n",
      "bernoulli_glm\n",
      "Julia means = [0.01, 0.05, 0.09, 0.1, 0.08, 0.03, -0.0, -0.03, -0.02, -0.03]\n",
      "Python means = Float32[0.08, -0.01, -0.05, -0.07, -0.06, -0.03, -0.0, 0.04, 0.05, 0.04] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for task_name in String.(keys(task_priors))\n",
    "    n = 2000\n",
    "    jl_prior = task_priors[Symbol(task_name)]\n",
    "    jl_samples = sample_θ(jl_prior, n)\n",
    "    jl_mean = mean.(eachcol(jl_samples))\n",
    "    jl_cov = cov(jl_samples)\n",
    "\n",
    "    py_prior = sbibm.get_task(task_name).get_prior()\n",
    "    py_samples = py_prior(n).numpy();\n",
    "    py_mean = mean.(eachcol(py_samples))\n",
    "    py_cov = cov(py_samples)\n",
    "    \n",
    "    println(task_name)\n",
    "    println(\"Julia means = $(round.(jl_mean; digits = 2))\")\n",
    "    println(\"Python means = $(round.(py_mean; digits = 2)) \\n\")\n",
    "    \n",
    "    @assert size(py_mean) == size(jl_mean)\n",
    "    @assert isapprox(py_mean, jl_mean; rtol = 2)\n",
    "    @assert isapprox(py_cov, jl_cov; rtol = 0.7)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the variance ratios between prior and posterior\n",
    "To get a good idea for the defualt proposal we can compare the variance of the posterior to the prior on the tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gaussian_linear: 0.4998179227113724\n",
      "gaussian_linear_uniform: 0.2024229694157839\n",
      "gaussian_mixture: 0.010489855706691741\n",
      "sir: 0.1139117874962996\n",
      "bernoulli_glm: 0.0981549893539872\n",
      "The mean variance ratio is 0.18495950493682695\n"
     ]
    }
   ],
   "source": [
    "mean_ratio = begin\n",
    "    ratios = []\n",
    "    for task_name in String.(keys(task_priors))\n",
    "        n = 2000\n",
    "        jl_prior = task_priors[Symbol(task_name)]\n",
    "        prior_var = diag(cov(jl_prior))\n",
    "        py_task = sbibm.get_task(task_name)\n",
    "        posterior_samples = py_task.get_reference_posterior_samples(1).numpy()\n",
    "        posterior_var = diag(cov(posterior_samples))\n",
    "        ratio = mean(posterior_var ./ prior_var)\n",
    "        println(task_name, \": \", ratio)\n",
    "        push!(ratios, ratio)\n",
    "    end\n",
    "    mean(ratios)\n",
    "end\n",
    "\n",
    "println(\"The mean variance ratio is $(mean_ratio)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_jl_simulator(task)   \n",
    "    py_simulator = task.get_simulator()\n",
    "    simulator(θ::Vector{Float64}) = begin\n",
    "        θ = torch.tensor(θ, dtype = torch.float32)\n",
    "        x = py_simulator(θ)\n",
    "        convert(Vector{Float64}, vec(x.numpy()))\n",
    "    end\n",
    "    simulator\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct JuliaTask\n",
    "    name\n",
    "    simulator\n",
    "    prior\n",
    "    s_true\n",
    "    obs_seed\n",
    "end\n",
    "\n",
    "function JuliaTask(python_task, obs_seed::Integer)\n",
    "    name = python_task.name\n",
    "    simulator = get_jl_simulator(python_task)\n",
    "    prior = task_priors[Symbol(name)]\n",
    "    s_true = vec(python_task.get_observation(obs_seed).numpy())\n",
    "    s_true = convert(Vector{Float64}, s_true)\n",
    "    JuliaTask(name, simulator, prior, s_true, obs_seed)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop through tasks and run the Riemannian ULA algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "const n_steps = 4000\n",
    "const n_sim = 1000  # at each mcmc iteration\n",
    "const n_burn = 1000;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Task = gaussian_linear\n",
      "└ @ Main In[8]:6\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:24:43\u001b[39m\n",
      "┌ Info: Task = gaussian_linear_uniform\n",
      "└ @ Main In[8]:6\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:27:31\u001b[39m\n",
      "┌ Info: Task = gaussian_mixture\n",
      "└ @ Main In[8]:6\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:11:47\u001b[39m\n",
      "┌ Info: Task = sir\n",
      "└ @ Main In[8]:6\n",
      "\u001b[32mProgress:  86%|███████████████████████████████████▎     |  ETA: 0:19:24\u001b[39m┌ Warning: GLM did not converge. Corresponding variance set to sample\n",
      "│ covariance.\n",
      "└ @ SyntheticLikelihood /home/dw16200/.julia/packages/SyntheticLikelihood/i1EbA/src/glm_local_regression.jl:69\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 2:17:43\u001b[39m\n",
      "┌ Info: Task = bernoulli_glm\n",
      "└ @ Main In[8]:6\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:24:45\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"./results/rula.csv\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algorithm = \"rula\"\n",
    "tasks = []\n",
    "run_times = []\n",
    "\n",
    "for (i, task_name) in enumerate(String.(keys(task_priors)))\n",
    "    @info \"Task = $(task_name)\"\n",
    "\n",
    "    Random.seed!(i)\n",
    "    pytask = sbibm.get_task(task_name)\n",
    "    jltask = JuliaTask(pytask, 1)\n",
    "    \n",
    "    @unpack simulator, prior, s_true, obs_seed = jltask\n",
    "    \n",
    "    init_θ = sample_θ(prior)\n",
    "\n",
    "    local_posterior = LocalPosterior(;\n",
    "      simulator, s_true, n_sim, prior,\n",
    "    )\n",
    "    \n",
    "    rula = RiemannianULA(0.2)\n",
    "    \n",
    "    time = @elapsed data = run_sampler!(rula, local_posterior; init_θ, n_steps)\n",
    "    open(\"./samples/$(task_name)_$(algorithm).txt\", \"w\") do io\n",
    "        writedlm(io, data.θ[(n_burn+1):end, :])\n",
    "    end\n",
    "                             \n",
    "    push!(tasks, task_name)\n",
    "    push!(run_times, time)\n",
    "\n",
    "end\n",
    "\n",
    "df = DataFrame(task = tasks, run_time = run_times)\n",
    "CSV.write(\"./results/$(algorithm).csv\", df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop through tasks and run basic Bayesian Synthetic Likelihood\n",
    "Below we use standard synthetic likelihood. We use a burn in of 1000 \"steps\" (either accepted or rejected), and then use the empirical covariance matrix of the last 75% of samples of the burn in to form the proposal distribution for the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Task = sir\n",
      "└ @ Main In[11]:8\n",
      "\u001b[32mProgress:   9%|███▋                                     |  ETA: 0:31:24\u001b[39m"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "None of the summary statistics had any variance.",
     "output_type": "error",
     "traceback": [
      "None of the summary statistics had any variance.",
      "",
      "Stacktrace:",
      "  [1] error(s::String)",
      "    @ Base ./error.jl:33",
      "  [2] remove_invariant(s::Matrix{Float64}, s_true::Vector{Float64})",
      "    @ SyntheticLikelihood ~/.julia/packages/SyntheticLikelihood/i1EbA/src/utils.jl:81",
      "  [3] obj_grad_hess(basic_posterior::BasicPosterior, θ::Vector{Float64})",
      "    @ SyntheticLikelihood ~/.julia/packages/SyntheticLikelihood/i1EbA/src/local_regression.jl:120",
      "  [4] update!(sampler::RWMetropolis, local_approximation::BasicPosterior, state::SyntheticLikelihood.RWMetropolisState)",
      "    @ SyntheticLikelihood ~/.julia/packages/SyntheticLikelihood/i1EbA/src/samplers.jl:194",
      "  [5] macro expansion",
      "    @ ~/.julia/packages/SyntheticLikelihood/i1EbA/src/samplers.jl:134 [inlined]",
      "  [6] macro expansion",
      "    @ ~/.julia/packages/ProgressMeter/0ub8y/src/ProgressMeter.jl:773 [inlined]",
      "  [7] run_sampler!(sampler::RWMetropolis, local_approximation::BasicPosterior; init_θ::Vector{Float64}, n_steps::Int64, collect_data::Vector{Symbol}, progress::Bool)",
      "    @ SyntheticLikelihood ~/.julia/packages/SyntheticLikelihood/i1EbA/src/samplers.jl:132",
      "  [8] macro expansion",
      "    @ timing.jl:287 [inlined]",
      "  [9] top-level scope",
      "    @ In[11]:22",
      " [10] eval",
      "    @ ./boot.jl:360 [inlined]",
      " [11] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "    @ Base ./loading.jl:1094"
     ]
    }
   ],
   "source": [
    "algorithm = \"bsl\"\n",
    "tasks = []\n",
    "run_times = []\n",
    "accecptance_rates = []\n",
    "\n",
    "\n",
    "for (i, task_name) in enumerate(String.(keys(task_priors)))\n",
    "    @info \"Task = $(task_name)\"\n",
    "\n",
    "    Random.seed!(i)\n",
    "    pytask = sbibm.get_task(task_name)\n",
    "    jltask = JuliaTask(pytask, 1)\n",
    "    \n",
    "    @unpack simulator, prior, s_true, obs_seed = jltask\n",
    "    \n",
    "    init_θ = sample_θ(prior)\n",
    "    \n",
    "    # Burn in 1000 \"steps\" and 0.2*covariance of the prior\n",
    "    rwm = RWMetropolis(MvNormal(0.2*cov(prior)))\n",
    "    basic_posterior = BasicPosterior(;simulator, s_true, n_sim, prior)\n",
    "    \n",
    "    time1 = @elapsed data = run_sampler!(rwm, basic_posterior; init_θ, n_steps = n_burn, collect_data = [:θ, :accepted])\n",
    "\n",
    "    \n",
    "    burn_in_θ = data.θ[data.accepted, :]\n",
    "    quarter = round(Int64, size(burn_in_θ, 1)*0.25)  \n",
    "    new_Σ = cov(burn_in_θ[quarter:end, :])\n",
    "        \n",
    "    # Actual run\n",
    "    rwm = RWMetropolis(MvNormal(new_Σ))\n",
    "    basic_posterior = BasicPosterior(;simulator, s_true, n_sim, prior)\n",
    "    time2 = @elapsed data = run_sampler!(rwm, basic_posterior; init_θ, n_steps = n_steps - n_burn, collect_data = [:θ, :accepted])\n",
    "     \n",
    "    open(\"./samples/$(task_name)_$(algorithm).txt\", \"w\") do io\n",
    "            writedlm(io, data.θ[data.accepted, :])\n",
    "    end\n",
    "    \n",
    "    push!(tasks, task_name)\n",
    "    push!(run_times, time1 + time2)\n",
    "    push!(accecptance_rates, sum(data.accepted)/length(data.accepted))\n",
    "end\n",
    "\n",
    "df = DataFrame(task = tasks, run_time = run_times, acceptance_rate = accecptance_rates)\n",
    "CSV.write(\"./results/$(algorithm).csv\", df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using DelimitedFiles\n",
    "#using GLM\n",
    "#X = readdlm(\"X.txt\")\n",
    "#y = readdlm(\"y.txt\")\n",
    "#y = reshape(y, length(y))\n",
    "#glm(X, y, Gamma(), LogLink(), maxiter=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.0",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
