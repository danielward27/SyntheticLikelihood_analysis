{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note this script needs a conda environment with sbibm installed, e.g.:\n",
    "\n",
    "# using Conda\n",
    "# using Pkg\n",
    "\n",
    "# ENV[\"PYTHON\"] = \"\"\n",
    "# Pkg.build(\"PyCall\")\n",
    "\n",
    "# Conda.pip_interop(true)\n",
    "# Conda.pip(\"install\", \"sbibm\")\n",
    "\n",
    "# Make sure up to date\n",
    "# Pkg.rm(\"SyntheticLikelihood\")\n",
    "# Pkg.add(url=\"https://github.com/danielward27/SyntheticLikelihood.jl\")\n",
    "using SyntheticLikelihood\n",
    "using PyCall\n",
    "using Distributions\n",
    "using DelimitedFiles\n",
    "using Random\n",
    "using Parameters\n",
    "using LinearAlgebra\n",
    "using DataFrames\n",
    "using CSV\n",
    "\n",
    "sbibm = pyimport(\"sbibm\")\n",
    "torch = pyimport(\"torch\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert stuff from python to julia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"gaussian_linear\", \"gaussian_linear_uniform\", \"gaussian_mixture\", \"bernoulli_glm\")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_priors = include(\"task_priors.jl\")\n",
    "String.(keys(task_priors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rough test that prior conversion looks right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gaussian_linear\n",
      "Julia means = [0.01, -0.0, 0.01, -0.01, -0.0, 0.02, -0.0, -0.01, 0.0, 0.0]\n",
      "Python means = Float32[0.0, 0.0, 0.0, 0.0, 0.01, -0.01, -0.01, -0.01, 0.0, -0.0] \n",
      "\n",
      "gaussian_linear_uniform\n",
      "Julia means = [0.0, -0.0, -0.0, -0.01, 0.02, 0.01, -0.03, 0.01, -0.02, -0.02]\n",
      "Python means = Float32[-0.01, -0.0, -0.02, -0.01, -0.01, -0.01, -0.0, 0.02, 0.01, -0.0] \n",
      "\n",
      "gaussian_mixture\n",
      "Julia means = [0.12, 0.08]\n",
      "Python means = Float32[0.11, 0.17] \n",
      "\n",
      "bernoulli_glm\n",
      "Julia means = [-0.0, -0.0, -0.02, -0.03, -0.04, -0.05, -0.03, -0.01, 0.01, 0.03]\n",
      "Python means = Float32[0.04, -0.03, -0.03, -0.01, 0.0, -0.03, -0.05, -0.02, 0.0, 0.03] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for task_name in String.(keys(task_priors))\n",
    "    n = 2000\n",
    "    jl_prior = task_priors[Symbol(task_name)]\n",
    "    jl_samples = sample_θ(jl_prior, n)\n",
    "    jl_mean = mean.(eachcol(jl_samples))\n",
    "    jl_cov = cov(jl_samples)\n",
    "\n",
    "    py_prior = sbibm.get_task(task_name).get_prior()\n",
    "    py_samples = py_prior(n).numpy();\n",
    "    py_mean = mean.(eachcol(py_samples))\n",
    "    py_cov = cov(py_samples)\n",
    "    \n",
    "    println(task_name)\n",
    "    println(\"Julia means = $(round.(jl_mean; digits = 2))\")\n",
    "    println(\"Python means = $(round.(py_mean; digits = 2)) \\n\")\n",
    "    \n",
    "    @assert size(py_mean) == size(jl_mean)\n",
    "    @assert isapprox(py_mean, jl_mean; rtol = 2)\n",
    "    @assert isapprox(py_cov, jl_cov; rtol = 0.7)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the variance ratios between prior and posterior\n",
    "To get a good idea for the defualt proposal we can compare the variance of the posterior to the prior on the tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gaussian_linear: 0.4998179227113724\n",
      "gaussian_linear_uniform: 0.2024229694157839\n",
      "gaussian_mixture: 0.010489855706691741\n",
      "bernoulli_glm: 0.0981549893539872\n",
      "The mean variance ratio is 0.20272143429695877\n"
     ]
    }
   ],
   "source": [
    "mean_ratio = begin\n",
    "    ratios = []\n",
    "    for task_name in String.(keys(task_priors))\n",
    "        n = 2000\n",
    "        jl_prior = task_priors[Symbol(task_name)]\n",
    "        prior_var = diag(cov(jl_prior))\n",
    "        py_task = sbibm.get_task(task_name)\n",
    "        posterior_samples = py_task.get_reference_posterior_samples(1).numpy()\n",
    "        posterior_var = diag(cov(posterior_samples))\n",
    "        ratio = mean(posterior_var ./ prior_var)\n",
    "        println(task_name, \": \", ratio)\n",
    "        push!(ratios, ratio)\n",
    "    end\n",
    "    mean(ratios)\n",
    "end\n",
    "\n",
    "println(\"The mean variance ratio is $(mean_ratio)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_jl_simulator(task)   \n",
    "    py_simulator = task.get_simulator()\n",
    "    simulator(θ::Vector{Float64}) = begin\n",
    "        θ = torch.tensor(θ, dtype = torch.float32)\n",
    "        x = py_simulator(θ)\n",
    "        convert(Vector{Float64}, vec(x.numpy()))\n",
    "    end\n",
    "    simulator\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct JuliaTask\n",
    "    name\n",
    "    simulator\n",
    "    prior\n",
    "    s_true\n",
    "    obs_seed\n",
    "end\n",
    "\n",
    "function JuliaTask(python_task, obs_seed::Integer)\n",
    "    name = python_task.name\n",
    "    simulator = get_jl_simulator(python_task)\n",
    "    prior = task_priors[Symbol(name)]\n",
    "    s_true = vec(python_task.get_observation(obs_seed).numpy())\n",
    "    s_true = convert(Vector{Float64}, s_true)\n",
    "    JuliaTask(name, simulator, prior, s_true, obs_seed)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get starting parameters\n",
    "To get the starting parameters I will sample 1000 sets of parameters and take the mean of all the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_starting_params (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function get_starting_params(prior::Prior)\n",
    "    mean.(eachcol(sample_θ(prior, 1000)))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop through tasks and run the Riemannian ULA algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "const n_steps = 4000\n",
    "const n_sim = 1000  # at each mcmc iteration\n",
    "const n_burn = 1000;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Task = gaussian_linear\n",
      "└ @ Main In[25]:7\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:27:27\u001b[39m\n",
      "┌ Info: Task = gaussian_linear_uniform\n",
      "└ @ Main In[25]:7\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:26:19\u001b[39m\n",
      "┌ Info: Task = gaussian_mixture\n",
      "└ @ Main In[25]:7\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:12:14\u001b[39m\n",
      "┌ Info: Task = sir\n",
      "└ @ Main In[25]:7\n",
      "┌ Warning: GLM did not converge. Corresponding variance set to sample\n",
      "│ covariance.\n",
      "└ @ SyntheticLikelihood /home/dw16200/.julia/packages/SyntheticLikelihood/vVDAq/src/glm_local_regression.jl:69\n",
      "\u001b[32mProgress:  13%|█████▍                                   |  ETA: 2:14:51\u001b[39m┌ Warning: sir failed!\n",
      "└ @ Main In[25]:25\n",
      "┌ Info: Task = bernoulli_glm\n",
      "└ @ Main In[25]:7\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:27:19\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"./results/rula_step_size_05.csv\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algorithm = \"rula\"\n",
    "tasks = []\n",
    "run_times = []\n",
    "errors = []\n",
    "\n",
    "for (i, task_name) in enumerate(String.(keys(task_priors)))\n",
    "    @info \"Task = $(task_name)\"\n",
    "\n",
    "    Random.seed!(i)\n",
    "    pytask = sbibm.get_task(task_name)\n",
    "    jltask = JuliaTask(pytask, 1)\n",
    "    @unpack simulator, prior, s_true, obs_seed = jltask\n",
    "    \n",
    "    init_θ = get_starting_params(prior)\n",
    "\n",
    "    local_posterior = LocalPosterior(;\n",
    "      simulator, s_true, n_sim, prior,\n",
    "    )\n",
    "    \n",
    "    rula = RiemannianULA(0.5)\n",
    "    \n",
    "    time = @elapsed data = try\n",
    "        run_sampler!(rula, local_posterior; init_θ, n_steps)\n",
    "    catch e\n",
    "        @warn \"$(task_name) failed!\"\n",
    "        push!(errors, e)\n",
    "        continue\n",
    "    end\n",
    "    \n",
    "    open(\"./samples/$(task_name)_$(algorithm).txt\", \"w\") do io\n",
    "        writedlm(io, data.θ[(n_burn+1):end, :])\n",
    "    end\n",
    "                             \n",
    "    push!(tasks, task_name)\n",
    "    push!(run_times, time)\n",
    "\n",
    "end\n",
    "\n",
    "df = DataFrame(task = tasks, run_time = run_times)\n",
    "CSV.write(\"./results/$(algorithm).csv\", df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop through tasks and run basic Bayesian Synthetic Likelihood\n",
    "Below we use standard synthetic likelihood. We use a burn in of 1000 steps (either accepted or rejected), and then use the empirical covariance matrix of the last 75% of samples of the burn in to form the proposal distribution for the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Task = gaussian_linear\n",
      "└ @ Main In[32]:8\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:05:03\u001b[39m\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:15:48\u001b[39m\n",
      "┌ Info: Task = gaussian_linear_uniform\n",
      "└ @ Main In[32]:8\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:08:03\u001b[39m\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:17:52\u001b[39m\n",
      "┌ Info: Task = gaussian_mixture\n",
      "└ @ Main In[32]:8\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:02:45\u001b[39m\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:08:06\u001b[39m\n",
      "┌ Info: Task = bernoulli_glm\n",
      "└ @ Main In[32]:8\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:04:51\u001b[39m\n",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:14:26\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"./results/rwm.csv\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algorithm = \"rwm\"\n",
    "tasks = []\n",
    "run_times = []\n",
    "accecptance_rates = []\n",
    "errors = []\n",
    "\n",
    "for (i, task_name) in enumerate(String.(keys(task_priors)))\n",
    "    @info \"Task = $(task_name)\"\n",
    "\n",
    "    Random.seed!(i)\n",
    "    pytask = sbibm.get_task(task_name)\n",
    "    jltask = JuliaTask(pytask, 1)\n",
    "    \n",
    "    @unpack simulator, prior, s_true, obs_seed = jltask\n",
    "    \n",
    "    init_θ = get_starting_params(prior)\n",
    "    \n",
    "    # Burn in 1000 \"steps\" and 0.2*covariance of the prior\n",
    "    rwm = RWMetropolis(MvNormal(0.2*cov(prior)))\n",
    "    basic_posterior = BasicPosterior(;simulator, s_true, n_sim, prior)\n",
    "    \n",
    "    time1 = @elapsed data = try\n",
    "         run_sampler!(rwm, basic_posterior; init_θ, n_steps = n_burn, collect_data = [:θ, :accepted])\n",
    "    catch e\n",
    "        @warn \"$(task_name) failed!\"\n",
    "        push!(errors, e)\n",
    "    end\n",
    "\n",
    "    burn_in_θ = data.θ\n",
    "    quarter = round(Int64, size(burn_in_θ, 1)*0.25)  \n",
    "    new_Σ = (2.38^2)*cov(burn_in_θ[quarter:end, :]) ./ size(burn_in_θ, 2)\n",
    "    init_θ = burn_in_θ[end, :]\n",
    "        \n",
    "    # Actual run\n",
    "    rwm = RWMetropolis(MvNormal(new_Σ))\n",
    "    basic_posterior = BasicPosterior(;simulator, s_true, n_sim, prior)\n",
    "    time2 = @elapsed data = run_sampler!(rwm, basic_posterior; init_θ, n_steps = n_steps - n_burn, collect_data = [:θ, :accepted])\n",
    "     \n",
    "    open(\"./samples/$(task_name)_$(algorithm).txt\", \"w\") do io\n",
    "            writedlm(io, data.θ)\n",
    "    end\n",
    "    \n",
    "    push!(tasks, task_name)\n",
    "    push!(run_times, time1 + time2)\n",
    "    push!(accecptance_rates, sum(data.accepted)/length(data.accepted))\n",
    "end\n",
    "\n",
    "df = DataFrame(task = tasks, run_time = run_times, acceptance_rate = accecptance_rates)\n",
    "CSV.write(\"./results/$(algorithm).csv\", df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using DelimitedFiles\n",
    "#using GLM\n",
    "#X = readdlm(\"X.txt\")\n",
    "#y = readdlm(\"y.txt\")\n",
    "#y = reshape(y, length(y))\n",
    "#glm(X, y, Gamma(), LogLink(), maxiter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4093490824269389e22"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean(LogNormal(1,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run times on simple gaussian example\n",
    "Check how the run time scales with the number of parameters, and the number of summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Any[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algorithm = \"rula\"\n",
    "tasks = []\n",
    "run_times = []\n",
    "errors = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "LoadError: UndefVarError: @profile not defined\nin expression starting at In[12]:17",
     "output_type": "error",
     "traceback": [
      "LoadError: UndefVarError: @profile not defined\nin expression starting at In[12]:17",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ :0",
      " [2] eval",
      "   @ ./boot.jl:360 [inlined]",
      " [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1094"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "task_name = String.(keys(task_priors))[1]\n",
    "Random.seed!(i)\n",
    "pytask = sbibm.get_task(task_name)\n",
    "jltask = JuliaTask(pytask, 1)\n",
    "@unpack simulator, prior, s_true, obs_seed = jltask\n",
    "\n",
    "init_θ = get_starting_params(prior)\n",
    "\n",
    "local_posterior = LocalPosterior(;\n",
    "      simulator, s_true, n_sim, prior,\n",
    ")\n",
    "\n",
    "rula = RiemannianULA(0.5)\n",
    "    \n",
    "   \n",
    "@profile run_sampler!(rula, local_posterior; init_θ, n_steps)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: n_steps not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: n_steps not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[2]:1",
      " [2] eval",
      "   @ ./boot.jl:360 [inlined]",
      " [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1094"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for (i, task_name) in enumerate(String.(keys(task_priors)))\n",
    "    @info \"Task = $(task_name)\"\n",
    "\n",
    "    Random.seed!(i)\n",
    "    pytask = sbibm.get_task(task_name)\n",
    "    jltask = JuliaTask(pytask, 1)\n",
    "    @unpack simulator, prior, s_true, obs_seed = jltask\n",
    "    \n",
    "    init_θ = get_starting_params(prior)\n",
    "\n",
    "    local_posterior = LocalPosterior(;\n",
    "      simulator, s_true, n_sim, prior,\n",
    "    )\n",
    "    \n",
    "    rula = RiemannianULA(0.5)\n",
    "    \n",
    "   \n",
    "        run_sampler!(rula, local_posterior; init_θ, n_steps)\n",
    "\n",
    "    \n",
    "    open(\"./samples/$(task_name)_$(algorithm).txt\", \"w\") do io\n",
    "        writedlm(io, data.θ[(n_burn+1):end, :])\n",
    "    end\n",
    "                             \n",
    "    push!(tasks, task_name)\n",
    "    push!(run_times, time)\n",
    "\n",
    "end\n",
    "\n",
    "df = DataFrame(task = tasks, run_time = run_times)\n",
    "CSV.write(\"./results/$(algorithm).csv\", df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.0",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
